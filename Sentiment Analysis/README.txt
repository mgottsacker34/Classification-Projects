	For the shared big data competition, I used the docclass.py file from the book and started with the basic outline of the classify.py file.  I created another classifying file (classify-test.py) to test my program’s success rate without submitting it to Kaggle.  My program tested the first 1500 tweets from both the eu-good.txt file and the eu-bad.txt file after training on every other tweet.  It ran the classifier on each tweet.  Since I knew the actual sentiment values of my test data, I could find the percentage right achieved by my program.  I obtained a comma-separated list of all the words used in the training data along with their frequencies and used that information to help improve my classifier.  I created a list (rarewords) that holds all of the words I wanted to use as training data.  Then, I looped through the frequency file and added all of the words to rarewords except those that occurred only once and words like “and” or “a” in the Basque language.  I experimented with not including various words and not including words that do only occur twice, thrice, or four times, but my test data yielded the best results when the words that only occur once and a handful of other words were excluded.  Next, my program reads lines from testing data and classifies them as positive or negative (‘P’ or ‘N’) using the Fisher Classifier from Chapter 6 of Programming Collective Intelligence.  I found that this method of classification produced a much higher percentage correct output for my testing data than the Naïve Bayes Classifier did for this dataset.
